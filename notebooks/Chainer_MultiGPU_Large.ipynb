{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Create a data-augmentor + data-preprocessing class\n",
    "# Using cv2 not pil and maybe using https://github.com/aleju/imgaug\n",
    "# Will ensure that pre-processing + augmentation standardised across frameworks\n",
    "# Also maybe PIL is bottlenecking pytorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import optimizers, cuda, dataset, training\n",
    "from chainer.training import extensions, StandardUpdater\n",
    "\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "from common.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Improvement\n",
    "# 1. Auto-tune\n",
    "# This adds very little now .. not sure if True by default?\n",
    "chainer.global_config.autotune = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "Chainer:  3.4.0\n",
      "CuPy:  2.4.0\n",
      "Numpy:  1.14.1\n",
      "GPU:  ['Tesla P100-PCIE-16GB', 'Tesla P100-PCIE-16GB']\n",
      "CUDA Version 8.0.61\n",
      "CuDNN Version  6.0.21\n",
      "CPUs:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"Chainer: \", chainer.__version__)\n",
    "print(\"CuPy: \", chainer.cuda.cupy.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"GPU: \", get_gpu_name())\n",
    "print(get_cuda_version())\n",
    "print(\"CuDNN Version \", get_cudnn_version())\n",
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "print(\"CPUs: \", CPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-set\n",
    "# Note if NUM_GPUS > 1 then MULTI_GPU = True and ALL GPUs will be used\n",
    "# Set below to affect batch-size\n",
    "# E.g. 1 GPU = 64, 2 GPUs = 64*2, 4 GPUs = 64*4\n",
    "# Note that the effective learning-rate will be decreased this way\n",
    "NUM_GPUS = 1 # Scaling factor for batch\n",
    "MULTI_GPU=NUM_GPUS>1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "CLASSES = 14\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "LR = 0.0001  # Effective learning-rate will decrease as BATCHSIZE rises\n",
    "EPOCHS = 5\n",
    "BATCHSIZE = 64*NUM_GPUS\n",
    "IMAGENET_RGB_MEAN =  np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_RGB_SD =  np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "TOT_PATIENT_NUMBER = 30805  # From data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestxray/images chestxray/Data_Entry_2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "CSV_DEST = \"chestxray\"\n",
    "IMAGE_FOLDER = os.path.join(CSV_DEST, \"images\")\n",
    "LABEL_FILE = os.path.join(CSV_DEST, \"Data_Entry_2017.csv\")\n",
    "print(IMAGE_FOLDER, LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please make sure to download\n",
      "https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\n",
      "Data already exists\n",
      "CPU times: user 678 ms, sys: 202 ms, total: 880 ms\n",
      "Wall time: 881 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Download data\n",
    "print(\"Please make sure to download\")\n",
    "print(\"https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\")\n",
    "download_data_chextxray(CSV_DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayData(dataset.DatasetMixin):\n",
    "    def __init__(self, patient_ids, img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, augmentation=None):\n",
    "        # Read labels-csv\n",
    "        df = pd.read_csv(lbl_file)\n",
    "        \n",
    "        # Split labels on unfiltered data\n",
    "        df_label = df['Finding Labels'].str.split(\n",
    "            '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "        \n",
    "        # Filter by patient-ids (both)\n",
    "        df_label['Patient ID'] = df['Patient ID']\n",
    "        df_label = df_label[df_label['Patient ID'].isin(patient_ids)]\n",
    "        df = df[df['Patient ID'].isin(patient_ids)]\n",
    "        \n",
    "        # Remove unncessary columns\n",
    "        df_label.drop(['Patient ID','No Finding'], axis=1, inplace=True)  \n",
    "        \n",
    "        # List of images (full-path)\n",
    "        self.img_locs =  df['Image Index'].map(lambda im: os.path.join(img_dir, im)).values\n",
    "        # One-hot encoded labels (float32 for BCE loss)\n",
    "        self.labels = df_label.values\n",
    "        \n",
    "        # Processing\n",
    "        self.augmentation = augmentation\n",
    "        print(\"Loaded {} labels and {} images\".format(len(self.labels), len(self.img_locs)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_locs)   \n",
    "    \n",
    "    def get_example(self, idx):\n",
    "        im_file = self.img_locs[idx]\n",
    "        # RGB Image\n",
    "        im = cv2.imread(im_file) \n",
    "        im_rgb = self._apply_data_preprocessing(im)\n",
    "        label = self.labels[idx]\n",
    "        if self.augmentation is not None:\n",
    "            im_rgb = self._apply_data_augmentation(im_rgb)\n",
    "        return np.array(im_rgb, dtype=np.float32), np.array(label, dtype=np.int32)\n",
    "    \n",
    "    def _apply_data_preprocessing(self, im, w=WIDTH, h=HEIGHT, \n",
    "                                  rgb_m=IMAGENET_RGB_MEAN, rgb_sd=IMAGENET_RGB_SD):\n",
    "        # REDO Method!\n",
    "        # BGR to RGB\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) \n",
    "        # Resize\n",
    "        im = cv2.resize(im, dsize=(w, h), interpolation=cv2.INTER_CUBIC)\n",
    "        # Channels first (3, 224, 224)\n",
    "        im = np.moveaxis(im, -1, 0)\n",
    "        # Normalise image with imagenet-values\n",
    "        # This is torch-model normalisation\n",
    "        # Ref: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py\n",
    "        im = im / 255.\n",
    "        im = (im - rgb_m[:, None, None]) / rgb_sd[:, None, None]\n",
    "        return im\n",
    "    \n",
    "    def _apply_data_augmentation(self, im):\n",
    "        # REDO Method!\n",
    "        # Random horizontal flip\n",
    "        if random.randint(0, 1):\n",
    "            im = cv2.flip(im, flipCode=1)\n",
    "        # Random rotation\n",
    "        # ...\n",
    "        # Random crop/zoom\n",
    "        # ...\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:21563 valid:3080 test:6162\n"
     ]
    }
   ],
   "source": [
    "# Training / Valid / Test split (70% / 10% / 20%)\n",
    "train_set, other_set = train_test_split(\n",
    "    range(1,TOT_PATIENT_NUMBER+1), train_size=0.7, test_size=0.3, shuffle=False)\n",
    "valid_set, test_set = train_test_split(other_set, train_size=1/3, test_size=2/3, shuffle=False)\n",
    "print(\"train:{} valid:{} test:{}\".format(\n",
    "    len(train_set), len(valid_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87306 labels and 87306 images\n",
      "Loaded 7616 labels and 7616 images\n",
      "Loaded 17198 labels and 17198 images\n"
     ]
    }
   ],
   "source": [
    "train_dataset = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=train_set, augmentation=True)\n",
    "valid_dataset = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=valid_set, augmentation=False)\n",
    "test_dataset  = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=test_set, augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chexnet(chainer.Chain):\n",
    "    def __init__(self, base_model, out_features=CLASSES):\n",
    "        super(chexnet, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.base_model = base_model\n",
    "            in_features = 2048  # How to get this programmatically?\n",
    "            self.classifier = L.Linear(in_features, out_features)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        h = self.base_model(x, layers=['pool5']) # How to get this programatically?\n",
    "        #return F.sigmoid(self.classifier(h))\n",
    "        return self.classifier(h)  # sigmoid applied in BCE function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol(model_name='densenet121'):\n",
    "    if model_name == 'resnet50':\n",
    "        base_model = chainer.links.ResNet50Layers(pretrained_model=\"auto\")\n",
    "    elif model_name == 'densenet121':\n",
    "        # https://github.com/chainer/chainer/issues/4426\n",
    "        raise ValueError(\"Densenet is not yet officially implemented in Chainer\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model-name\")\n",
    "    # Change last-layer\n",
    "    model = chexnet(base_model)\n",
    "    # CUDA\n",
    "    chainer.cuda.get_device(0).use()  # Make a specified GPU current\n",
    "    model.to_gpu()  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_symbol(sym, lr=LR):\n",
    "    opt = optimizers.Adam(alpha=lr, beta1=0.9, beta2=0.999)\n",
    "    opt.setup(sym)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 671 ms, sys: 399 ms, total: 1.07 s\n",
      "Wall time: 2.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load symbol\n",
    "chexnet_sym = get_symbol(model_name='resnet50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 545 µs, sys: 110 µs, total: 655 µs\n",
      "Wall time: 662 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load optimiser\n",
    "optimizer = init_symbol(chexnet_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-iterators\n",
    "train_iter = chainer.iterators.MultiprocessIterator(train_dataset, BATCHSIZE, n_processes=CPU_COUNT)\n",
    "valid_iter = chainer.iterators.MultiprocessIterator(valid_dataset, 16*BATCHSIZE, n_processes=CPU_COUNT)\n",
    "test_iter = chainer.iterators.MultiprocessIterator(test_dataset, 16*BATCHSIZE, n_processes=CPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level trainer-class for easy multi-gpu later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "updater = StandardUpdater(train_iter, optimizer, loss_func=F.sigmoid_cross_entropy, device=0)\n",
    "trainer = training.Trainer(updater, stop_trigger=(EPOCHS, 'epoch'))\n",
    "trainer.extend(extensions.ProgressBar(update_interval=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in main training loop: \n",
      "Invalid operation is performed in: SigmoidCrossEntropy (Forward)\n",
      "\n",
      "Expect: in_types[0].shape == in_types[1].shape\n",
      "Actual: (64, 3, 224, 224) != (64, 14)\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/training/trainer.py\", line 299, in run\n",
      "    update()\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/training/updater.py\", line 223, in update\n",
      "    self.update_core()\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/training/updater.py\", line 234, in update_core\n",
      "    optimizer.update(loss_func, *in_arrays)\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/optimizer.py\", line 541, in update\n",
      "    loss = lossfun(*args, **kwds)\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/functions/loss/sigmoid_cross_entropy.py\", line 168, in sigmoid_cross_entropy\n",
      "    return SigmoidCrossEntropy(normalize, reduce).apply((x, t))[0]\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/function_node.py\", line 230, in apply\n",
      "    self._check_data_type_forward(in_data)\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/function_node.py\", line 298, in _check_data_type_forward\n",
      "    self.check_type_forward(in_type)\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/functions/loss/sigmoid_cross_entropy.py\", line 33, in check_type_forward\n",
      "    x_type.shape == t_type.shape\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/utils/type_check.py\", line 524, in expect\n",
      "    expr.expect()\n",
      "  File \"/anaconda/envs/py35/lib/python3.5/site-packages/chainer/utils/type_check.py\", line 482, in expect\n",
      "    '{0} {1} {2}'.format(left, self.inv, right))\n",
      "Will finalize trainer extensions and updater before reraising the exception.\n"
     ]
    }
   ],
   "source": [
    "#trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one in train_iter:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 224, 224) (1, 14)\n"
     ]
    }
   ],
   "source": [
    "dta = np.expand_dims(one[0][0], 0)\n",
    "label = np.expand_dims(one[0][1], 0)\n",
    "print(dta.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-38daa4e9b27e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Forwards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchexnet_sym\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-b8e459ea8776>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pool5'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# How to get this programatically?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#return F.sigmoid(self.classifier(h))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sigmoid applied in BCE function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/chainer/links/connection/linear.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0min_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/chainer/functions/connection/linear.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(x, W, b)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "# Try forward and back pass\n",
    "data = cuda.to_gpu(dta)\n",
    "target = cuda.to_gpu(label)\n",
    "# Forwards\n",
    "output = chexnet_sym(data)\n",
    "# Loss\n",
    "loss = F.sigmoid_cross_entropy(output, target)\n",
    "chexnet_sym.cleargrads()\n",
    "# Back-prop\n",
    "loss.backward()\n",
    "optimizer.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
