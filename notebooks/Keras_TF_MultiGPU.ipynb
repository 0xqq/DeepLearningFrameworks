{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################\n",
    "# Summary\n",
    "# 1. Keras Multi-GPU example\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "os.environ['KERAS_BACKEND'] = \"tensorflow\"\n",
    "import keras as K  # 2.1.5 introduces breaking API change\n",
    "import tensorflow\n",
    "import multiprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.densenet import DenseNet121, preprocess_input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, Callback, ModelCheckpoint\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from common.utils import download_data_chextxray, get_imgloc_labels, get_train_valid_test_split\n",
    "from common.utils import compute_roc_auc, get_cuda_version, get_cudnn_version, get_gpu_name\n",
    "from common.params_dense import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "Keras:  2.1.4\n",
      "Numpy:  1.14.2\n",
      "Tensorflow:  1.6.0\n",
      "tensorflow\n",
      "channels_last\n",
      "GPU:  ['Tesla P100-PCIE-16GB', 'Tesla P100-PCIE-16GB', 'Tesla P100-PCIE-16GB', 'Tesla P100-PCIE-16GB']\n",
      "CUDA Version 9.0.176\n",
      "CuDNN Version  7.0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"Keras: \", K.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"Tensorflow: \", tensorflow.__version__)\n",
    "print(K.backend.backend())\n",
    "print(K.backend.image_data_format())\n",
    "print(\"GPU: \", get_gpu_name())\n",
    "print(get_cuda_version())\n",
    "print(\"CuDNN Version \", get_cudnn_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPUs:  24\n",
      "GPUs:  4\n"
     ]
    }
   ],
   "source": [
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "GPU_COUNT = len(get_gpu_name())\n",
    "print(\"CPUs: \", CPU_COUNT)\n",
    "print(\"GPUs: \", GPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestxray/images chestxray/Data_Entry_2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Model-params\n",
    "# Normalising done by keras.applications.densenet.preprocess_input()\n",
    "# Paths\n",
    "CSV_DEST = \"chestxray\"\n",
    "IMAGE_FOLDER = os.path.join(CSV_DEST, \"images\")\n",
    "LABEL_FILE = os.path.join(CSV_DEST, \"Data_Entry_2017.csv\")\n",
    "print(IMAGE_FOLDER, LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually scale to multi-gpu\n",
    "if MULTI_GPU:\n",
    "    LR *= GPU_COUNT \n",
    "    BATCHSIZE *= GPU_COUNT\n",
    "#Make sure channels-first (not last)\n",
    "K.backend.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please make sure to download\n",
      "https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\n",
      "Data already exists\n",
      "CPU times: user 708 ms, sys: 228 ms, total: 936 ms\n",
      "Wall time: 936 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Download data\n",
    "print(\"Please make sure to download\")\n",
    "print(\"https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\")\n",
    "download_data_chextxray(CSV_DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayData():\n",
    "    \n",
    "    def __init__(self, img_dir, lbl_file, patient_ids, \n",
    "                 width=WIDTH, height=HEIGHT, batch_size=BATCHSIZE, num_classes=CLASSES,\n",
    "                 shuffle=True, seed=None, augment=False):\n",
    "        \n",
    "        self.patient_ids = patient_ids\n",
    "        self.lbl_file = lbl_file\n",
    "        \n",
    "        # Hack for flow_from_directory to work, give it path above\n",
    "        self.child_path  = os.path.split(img_dir)[-1]\n",
    "        self.parent_path =  img_dir.replace(self.child_path,'')\n",
    "        \n",
    "        # Create ImageDataGenerator with DenseNet pre-processing\n",
    "        # imagenet_utils.preprocess_input(x, data_format, mode='torch')\n",
    "        if augment:\n",
    "            datagen = ImageDataGenerator(\n",
    "                horizontal_flip=True,\n",
    "                # Best match to?\n",
    "                # transforms.RandomResizedCrop(size=WIDTH),\n",
    "                zoom_range=0.2,  \n",
    "                rotation_range=10,\n",
    "                preprocessing_function=preprocess_input)\n",
    "        else:\n",
    "             datagen = ImageDataGenerator(preprocessing_function=preprocess_input)    \n",
    "\n",
    "        # Create flow-from-directory\n",
    "        flowgen = datagen.flow_from_directory(\n",
    "            directory=self.parent_path,  # hack: this is one directory up\n",
    "            target_size=(width, height),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            class_mode='binary')    \n",
    "        \n",
    "        # Override previously created classes variables\n",
    "        # filenames, classes\n",
    "        flowgen.filenames, flowgen.classes = get_imgloc_labels(\n",
    "            self.child_path, lbl_file, patient_ids)\n",
    "        # number of files\n",
    "        flowgen.n = len(flowgen.filenames)\n",
    "        # number of classes (not sure if this last one needed)\n",
    "        flowgen.num_classes = num_classes\n",
    "        \n",
    "        self.generator = flowgen\n",
    "        print(\"Loaded {} labels and {} images\".format(len(self.generator.classes), \n",
    "                                                      len(self.generator.filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:21563 valid:3080 test:6162\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set = get_train_valid_test_split(TOT_PATIENT_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 112120 images belonging to 1 classes.\n",
      "Loaded 87306 labels and 87306 images\n"
     ]
    }
   ],
   "source": [
    "train_dataset = XrayData(IMAGE_FOLDER, LABEL_FILE, train_set, augment=True).generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 112120 images belonging to 1 classes.\n",
      "Loaded 7616 labels and 7616 images\n",
      "Found 112120 images belonging to 1 classes.\n",
      "Loaded 17198 labels and 17198 images\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = XrayData(IMAGE_FOLDER, LABEL_FILE, valid_set, shuffle=False).generator\n",
    "test_dataset = XrayData(IMAGE_FOLDER, LABEL_FILE, test_set, shuffle=False).generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol(model_name='densenet121', out_features=CLASSES):\n",
    "    if model_name == 'densenet121':\n",
    "        model = DenseNet121(include_top=False, weights='imagenet', input_shape=(3, 224, 224), pooling='avg')\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model-name\")\n",
    "    # Add classifier to model FC-14\n",
    "    classifier = Dense(out_features, activation='sigmoid')(model.output)\n",
    "    model = Model(inputs=model.input, outputs=classifier)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_symbol(sym, lr=LR):\n",
    "    # BCE Loss since classes not mutually exclusive + Sigmoid FC-layer\n",
    "    sym.compile(\n",
    "        loss = \"binary_crossentropy\",\n",
    "        optimizer = Adam(lr, beta_1=0.9, beta_2=0.999, epsilon=None))\n",
    "    # Callbacks\n",
    "    sch = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
    "    #This doesnt work with Keras multi-gpu\n",
    "    #Don't want to add another hack to get it fixed\n",
    "    #chp = ModelCheckpoint('best_chexnet.pth.hdf5', monitor='val_loss', save_weights_only=False)\n",
    "    callbacks = [sch]\n",
    "    return sym, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Train CheXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 6.1 s, total: 1min 33s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if MULTI_GPU:\n",
    "    with tensorflow.device('/cpu:0'):\n",
    "        # Recommended to instantiate base model on CPU\n",
    "        # https://keras.io/utils/#multi_gpu_model\n",
    "        sym = get_symbol()\n",
    "    chexnet_sym = multi_gpu_model(sym, gpus=GPU_COUNT)\n",
    "else:\n",
    "    chexnet_sym = get_symbol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.7 ms, sys: 3.59 ms, total: 39.3 ms\n",
      "Wall time: 37.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load optimiser, loss\n",
    "model, callbacks = init_symbol(chexnet_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "342/342 [==============================] - 342s 999ms/step - loss: 0.1807 - val_loss: 0.1685\n",
      "Epoch 2/5\n",
      "342/342 [==============================] - 254s 742ms/step - loss: 0.1522 - val_loss: 0.1488\n",
      "Epoch 3/5\n",
      "342/342 [==============================] - 251s 733ms/step - loss: 0.1485 - val_loss: 0.1463\n",
      "Epoch 4/5\n",
      "342/342 [==============================] - ETA: 0s - loss: 0.145 - 245s 717ms/step - loss: 0.1458 - val_loss: 0.1481\n",
      "Epoch 5/5\n",
      "341/342 [============================>.] - ETA: 0s - loss: 0.1446Epoch 5/5\n",
      "342/342 [==============================] - 252s 738ms/step - loss: 0.1447 - val_loss: 0.1387\n",
      "CPU times: user 1h 6min 48s, sys: 23min 26s, total: 1h 30min 14s\n",
      "Wall time: 23min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f319d282860>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# 1 GPU - Main training loop: 51min 27s\n",
    "# 2 GPU - Main training loop: 32min 1s\n",
    "# 4 GPU - Main training loop: 23min 3s\n",
    "model.fit_generator(train_dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    workers=CPU_COUNT,  # Num of CPUs since multiprocessing\n",
    "                    use_multiprocessing=True,  # Faster than with threading\n",
    "                    validation_data=valid_dataset,\n",
    "                    max_queue_size=20)  # Default is 10 (most prob no difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Test CheXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for testing\n",
    "# Currently multi-GPU checkpointing is broken on Keras\n",
    "# For now use in-RAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 40s, sys: 1min 48s, total: 7min 29s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Evaluate\n",
    "# AUC: 0.8216\n",
    "y_guess = model.predict_generator(test_dataset, workers=CPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full AUC [0.8166704403329452, 0.8701978640353484, 0.8036644715384587, 0.8991700123597787, 0.8900824513691525, 0.9197848609229234, 0.7292038166231667, 0.8975747639269652, 0.6324781069481422, 0.8465972198647057, 0.7451801565774874, 0.8049089113120023, 0.7560819980737239, 0.8914456631015975]\n",
      "Validation AUC: 0.8216\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation AUC: {0:.4f}\".format(compute_roc_auc(test_dataset.classes, y_guess, CLASSES)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
