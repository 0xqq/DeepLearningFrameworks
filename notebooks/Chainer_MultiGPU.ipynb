{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Create a data-augmentor + data-preprocessing class\n",
    "# Using cv2 not pil and maybe using https://github.com/aleju/imgaug\n",
    "# Will ensure that pre-processing + augmentation standardised across frameworks\n",
    "# Also maybe PIL is bottlenecking pytorch?\n",
    "\n",
    "# Notes\n",
    "# http://corochann.com/install-chainer-1401.html#Install_NCCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import optimizers, cuda, dataset, training\n",
    "from chainer.training import extensions, updaters, StandardUpdater\n",
    "from chainer.dataset import concat_examples\n",
    "from chainer.links import ResNet50Layers\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import random\n",
    "#import cv2\n",
    "from common.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Improvement\n",
    "# 1. Auto-tune\n",
    "# This adds very little now .. not sure if True by default?\n",
    "chainer.global_config.autotune = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS:  linux\n",
      "Python:  3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "Chainer:  3.4.0\n",
      "CuPy:  2.4.0\n",
      "Numpy:  1.14.1\n",
      "GPU:  ['Tesla P100-PCIE-16GB', 'Tesla P100-PCIE-16GB']\n",
      "CUDA Version 8.0.61\n",
      "CuDNN Version  6.0.21\n",
      "CPUs:  12\n"
     ]
    }
   ],
   "source": [
    "print(\"OS: \", sys.platform)\n",
    "print(\"Python: \", sys.version)\n",
    "print(\"Chainer: \", chainer.__version__)\n",
    "print(\"CuPy: \", chainer.cuda.cupy.__version__)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"GPU: \", get_gpu_name())\n",
    "print(get_cuda_version())\n",
    "print(\"CuDNN Version \", get_cudnn_version())\n",
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "print(\"CPUs: \", CPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set \"num_gpus\"\n",
    "NUM_GPUS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_GPU=NUM_GPUS>1\n",
    "DEVICES=tuple(list(range(NUM_GPUS)))\n",
    "if MULTI_GPU:\n",
    "    from cupy.cuda import nccl  # Test that nccl works for multi-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "CLASSES = 14\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "LR = 0.0001  # Effective learning-rate will decrease as BATCHSIZE rises\n",
    "EPOCHS = 5\n",
    "#BATCHSIZE = 64*NUM_GPUS\n",
    "BATCHSIZE = 64  # Chainer auto scales batch\n",
    "IMAGENET_RGB_MEAN =  np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_RGB_SD =  np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "TOT_PATIENT_NUMBER = 30805  # From data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestxray/images chestxray/Data_Entry_2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "CSV_DEST = \"chestxray\"\n",
    "IMAGE_FOLDER = os.path.join(CSV_DEST, \"images\")\n",
    "LABEL_FILE = os.path.join(CSV_DEST, \"Data_Entry_2017.csv\")\n",
    "print(IMAGE_FOLDER, LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please make sure to download\n",
      "https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\n",
      "Data already exists\n",
      "CPU times: user 666 ms, sys: 225 ms, total: 890 ms\n",
      "Wall time: 889 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Download data\n",
    "print(\"Please make sure to download\")\n",
    "print(\"https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\")\n",
    "download_data_chextxray(CSV_DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayData(dataset.DatasetMixin):\n",
    "    def __init__(self, patient_ids, img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, augmentation=None):\n",
    "        # Read labels-csv\n",
    "        df = pd.read_csv(lbl_file)\n",
    "        \n",
    "        # Split labels on unfiltered data\n",
    "        df_label = df['Finding Labels'].str.split(\n",
    "            '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "        \n",
    "        # Filter by patient-ids (both)\n",
    "        df_label['Patient ID'] = df['Patient ID']\n",
    "        df_label = df_label[df_label['Patient ID'].isin(patient_ids)]\n",
    "        df = df[df['Patient ID'].isin(patient_ids)]\n",
    "        \n",
    "        # Remove unncessary columns\n",
    "        df_label.drop(['Patient ID','No Finding'], axis=1, inplace=True)  \n",
    "        \n",
    "        # List of images (full-path)\n",
    "        self.img_locs =  df['Image Index'].map(lambda im: os.path.join(img_dir, im)).values\n",
    "        # One-hot encoded labels (float32 for BCE loss)\n",
    "        self.labels = df_label.values\n",
    "        \n",
    "        # Processing\n",
    "        self.augmentation = augmentation\n",
    "        print(\"Loaded {} labels and {} images\".format(len(self.labels), len(self.img_locs)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_locs)   \n",
    "    \n",
    "    def get_example(self, idx):\n",
    "        im_file = self.img_locs[idx]\n",
    "        # RGB Image\n",
    "        im_rgb = Image.open(im_file).convert('RGB')\n",
    "        im_rgb = _apply_data_preprocessing(im_rgb)\n",
    "        label = self.labels[idx]\n",
    "        if self.augmentation is not None:\n",
    "            im_rgb = _apply_data_augmentation(im_rgb)\n",
    "        return np.array(im_rgb, dtype=np.float32), np.array(label, dtype=np.int32)\n",
    "    \n",
    "def _apply_data_preprocessing(rgb_im, w=WIDTH, h=HEIGHT, \n",
    "                              rgb_m=IMAGENET_RGB_MEAN, rgb_sd=IMAGENET_RGB_SD):\n",
    "    # REDO Method!\n",
    "    # https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py#L325\n",
    "    # Resize\n",
    "    rgb_im = rgb_im.resize((w, h), Image.NEAREST)\n",
    "    # Array\n",
    "    im = np.asarray(rgb_im, dtype=np.float32)\n",
    "    # (w, h, c) to (c, h, w)\n",
    "    im = im.transpose(2, 0, 1)\n",
    "    # Normalise image with imagenet-values\n",
    "    # This is torch-model normalisation\n",
    "    # Ref: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py\n",
    "    im = im / 255.\n",
    "    im = (im - rgb_m[:, None, None]) / rgb_sd[:, None, None]\n",
    "    return im\n",
    "\n",
    "def _apply_data_augmentation(x):\n",
    "    # REDO Method!\n",
    "    # Random horizontal flip\n",
    "    # col_axis = 2\n",
    "    if random.randint(0, 1):\n",
    "        x = x.swapaxes(2, 0)\n",
    "        x = x[::-1, ...]\n",
    "        x = x.swapaxes(0, 2)\n",
    "    # Random rotation\n",
    "    # ...\n",
    "    # Random crop/zoom\n",
    "    # ...\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:21563 valid:3080 test:6162\n"
     ]
    }
   ],
   "source": [
    "# Training / Valid / Test split (70% / 10% / 20%)\n",
    "train_set, other_set = train_test_split(\n",
    "    range(1,TOT_PATIENT_NUMBER+1), train_size=0.7, test_size=0.3, shuffle=False)\n",
    "valid_set, test_set = train_test_split(other_set, train_size=1/3, test_size=2/3, shuffle=False)\n",
    "print(\"train:{} valid:{} test:{}\".format(\n",
    "    len(train_set), len(valid_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 87306 labels and 87306 images\n",
      "Loaded 7616 labels and 7616 images\n",
      "Loaded 17198 labels and 17198 images\n"
     ]
    }
   ],
   "source": [
    "train_dataset = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=train_set, augmentation=True)\n",
    "valid_dataset = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=valid_set, augmentation=False)\n",
    "test_dataset  = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=test_set, augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chexnet(ResNet50Layers):\n",
    "    \n",
    "    def __init__(self, pretrained_model=\"auto\", basemodel_out='pool5', out_features=CLASSES):\n",
    "        super(chexnet, self).__init__(pretrained_model)\n",
    "        self._basemodel_out = basemodel_out\n",
    "        in_features = 2048\n",
    "        with self.init_scope():\n",
    "            self.classifier = L.Linear(in_features, out_features)\n",
    "    \n",
    "    def __call__(self, x, **kwargs):\n",
    "        h = super(chexnet, self).__call__(x, layers=[self._basemodel_out], **kwargs)[self._basemodel_out]\n",
    "        h = self.classifier(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol(model_name='densenet121', multi_gpu=MULTI_GPU):\n",
    "    if model_name == 'resnet50':\n",
    "        m = chexnet()\n",
    "    elif model_name == 'densenet121':\n",
    "        # https://github.com/chainer/chainer/issues/4426\n",
    "        raise ValueError(\"Densenet is not yet officially implemented in Chainer\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model-name\")\n",
    "    # CUDA\n",
    "    if not multi_gpu:\n",
    "        print(\"One GPU\")\n",
    "        chainer.cuda.get_device(0).use()  # Make a specified GPU current\n",
    "        m.to_gpu()  \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_symbol(sym, lr=LR):\n",
    "    opt = optimizers.Adam(alpha=lr, beta1=0.9, beta2=0.999)\n",
    "    opt.setup(sym)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_auc(data_gt, data_pd, full=True, classes=CLASSES):\n",
    "    roc_auc = []\n",
    "    for i in range(classes):\n",
    "        roc_auc.append(roc_auc_score(data_gt[:, i], data_pd[:, i]))\n",
    "    print(\"Full AUC\", roc_auc)\n",
    "    roc_auc = np.mean(roc_auc)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfun(x, t):\n",
    "    return F.sigmoid_cross_entropy(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 381 ms, sys: 80.1 ms, total: 461 ms\n",
      "Wall time: 461 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load symbol\n",
    "predictor = get_symbol(model_name='resnet50')\n",
    "chexnet_sym = L.Classifier(predictor, lossfun=lossfun)\n",
    "chexnet_sym.compute_accuracy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 718 µs, sys: 0 ns, total: 718 µs\n",
      "Wall time: 725 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load optimiser\n",
    "optimizer = init_symbol(chexnet_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-iterators\n",
    "# Open-CV runs on GPU by default!\n",
    "# Weird-bug to set shared_mem (set random value and adjust)\n",
    "if MULTI_GPU:\n",
    "    train_iters = [\n",
    "        chainer.iterators.MultiprocessIterator(\n",
    "            train_dataset, BATCHSIZE, shared_mem=700000, n_prefetch=10, n_processes=CPU_COUNT) \n",
    "        for i in chainer.datasets.split_dataset_n_random(train_dataset, len(DEVICES))]\n",
    "else:\n",
    "    train_iter = chainer.iterators.MultiprocessIterator(\n",
    "        train_dataset, BATCHSIZE, shared_mem=700000, n_prefetch=10, n_processes=CPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These can have a higher batch-size than train since no grads stored\n",
    "valid_iter = chainer.iterators.MultiprocessIterator(\n",
    "    valid_dataset, BATCHSIZE, shared_mem=700000, repeat=False, shuffle=False, n_prefetch=10, n_processes=CPU_COUNT)\n",
    "test_iter = chainer.iterators.MultiprocessIterator(\n",
    "    test_dataset, BATCHSIZE, shared_mem=700000, repeat=False, shuffle=False, n_prefetch=10, n_processes=CPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/chainer/training/updaters/multiprocess_parallel_updater.py:131: UserWarning: optimizer.eps is changed to 2e-08 by MultiprocessParallelUpdater for new batch size.\n",
      "  format(optimizer.eps))\n"
     ]
    }
   ],
   "source": [
    "# MultiprocessParallelUpdater requires NCCL.\n",
    "# https://github.com/nvidia/nccl#build--run\n",
    "if MULTI_GPU:\n",
    "    updater = updaters.MultiprocessParallelUpdater(train_iters, optimizer, devices=DEVICES)\n",
    "else:\n",
    "    updater = StandardUpdater(train_iter, optimizer, device=0)\n",
    "val_interval = (1, 'epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = training.Trainer(updater, stop_trigger=(EPOCHS, 'epoch'))\n",
    "trainer.extend(extensions.Evaluator(valid_iter, chexnet_sym, device=DEVICES[0]), trigger=val_interval)\n",
    "trainer.extend(extensions.LogReport(trigger=val_interval))\n",
    "trainer.extend(extensions.PrintReport(['epoch', 'iteration', 'main/loss', 'validation/main/loss']), trigger=val_interval)\n",
    "trainer.extend(extensions.ProgressBar(update_interval=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+-----------------------------------------------------------------------------+\n",
    "#| NVIDIA-SMI 384.111                Driver Version: 384.111                   |\n",
    "#|-------------------------------+----------------------+----------------------+\n",
    "#| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "#| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "#|===============================+======================+======================|\n",
    "#|   0  Tesla P100-PCIE...  Off  | 0000BC4B:00:00.0 Off |                    0 |\n",
    "#| N/A   56C    P0   196W / 250W |  16216MiB / 16276MiB |     99%      Default |\n",
    "#+-------------------------------+----------------------+----------------------+\n",
    "#|   1  Tesla P100-PCIE...  Off  | 0000D419:00:00.0 Off |                    0 |\n",
    "#| N/A   57C    P0   174W / 250W |   6651MiB / 16276MiB |    100%      Default |\n",
    "#+-------------------------------+----------------------+----------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[J     total [###...............................................]  7.33%\n",
      "this epoch [##################................................] 36.65%\n",
      "       500 iter, 0 epoch / 5 epochs\n",
      "       inf iters/sec. Estimated time to finish: 0:00:00.\n",
      "\u001b[4A\u001b[J     total [#######...........................................] 14.66%\n",
      "this epoch [####################################..............] 73.31%\n",
      "      1000 iter, 0 epoch / 5 epochs\n",
      "    2.8483 iters/sec. Estimated time to finish: 0:34:03.616602.\n",
      "\u001b[4Aepoch       iteration   main/loss   validation/main/loss\n",
      "\u001b[J1           1365        0.15378     0.141193              \n",
      "\u001b[J     total [##########........................................] 21.99%\n",
      "this epoch [####..............................................]  9.96%\n",
      "      1500 iter, 1 epoch / 5 epochs\n",
      "    2.7364 iters/sec. Estimated time to finish: 0:32:24.479975.\n",
      "\u001b[4A\u001b[J     total [##############....................................] 29.32%\n",
      "this epoch [#######################...........................] 46.61%\n",
      "      2000 iter, 1 epoch / 5 epochs\n",
      "    2.7726 iters/sec. Estimated time to finish: 0:28:58.743685.\n",
      "\u001b[4A\u001b[J     total [##################................................] 36.65%\n",
      "this epoch [#########################################.........] 83.26%\n",
      "      2500 iter, 1 epoch / 5 epochs\n",
      "    2.7911 iters/sec. Estimated time to finish: 0:25:48.074940.\n",
      "\u001b[4A\u001b[J2           2729        0.143742    0.14022               \n",
      "\u001b[J     total [#####################.............................] 43.98%\n",
      "this epoch [#########.........................................] 19.92%\n",
      "      3000 iter, 2 epoch / 5 epochs\n",
      "     2.759 iters/sec. Estimated time to finish: 0:23:04.850634.\n",
      "\u001b[4A\u001b[J     total [#########################.........................] 51.31%\n",
      "this epoch [############################......................] 56.57%\n",
      "      3500 iter, 2 epoch / 5 epochs\n",
      "    2.7734 iters/sec. Estimated time to finish: 0:19:57.368413.\n",
      "\u001b[4A\u001b[J     total [#############################.....................] 58.64%\n",
      "this epoch [##############################################....] 93.22%\n",
      "      4000 iter, 2 epoch / 5 epochs\n",
      "    2.7845 iters/sec. Estimated time to finish: 0:16:53.036710.\n",
      "\u001b[4A\u001b[J3           4093        0.134456    0.143522              \n",
      "\u001b[J     total [################################..................] 65.97%\n",
      "this epoch [##############....................................] 29.87%\n",
      "      4500 iter, 3 epoch / 5 epochs\n",
      "    2.7656 iters/sec. Estimated time to finish: 0:13:59.146774.\n",
      "\u001b[4A\u001b[J     total [####################################..............] 73.31%\n",
      "this epoch [#################################.................] 66.53%\n",
      "      5000 iter, 3 epoch / 5 epochs\n",
      "    2.7752 iters/sec. Estimated time to finish: 0:10:56.093676.\n",
      "\u001b[4A\u001b[J4           5457        0.123871    0.148153              \n",
      "\u001b[J     total [########################################..........] 80.64%\n",
      "this epoch [#.................................................]  3.18%\n",
      "      5500 iter, 4 epoch / 5 epochs\n",
      "    2.7608 iters/sec. Estimated time to finish: 0:07:58.406851.\n",
      "\u001b[4A\u001b[J     total [###########################################.......] 87.97%\n",
      "this epoch [###################...............................] 39.83%\n",
      "      6000 iter, 4 epoch / 5 epochs\n",
      "     2.769 iters/sec. Estimated time to finish: 0:04:56.415583.\n",
      "\u001b[4A\u001b[J     total [###############################################...] 95.30%\n",
      "this epoch [######################################............] 76.49%\n",
      "      6500 iter, 4 epoch / 5 epochs\n",
      "    2.7759 iters/sec. Estimated time to finish: 0:01:55.559432.\n",
      "\u001b[4A\u001b[J5           6821        0.10966     0.153518              \n",
      "\u001b[JCPU times: user 30min 54s, sys: 3min 24s, total: 34min 18s\n",
      "Wall time: 41min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 1 GPU = 3.1864 iters/sec\n",
    "# Wall time: 36m46s\n",
    "# 2 GPU = 2.6848 iters/sec\n",
    "# Wall time: 41min9s\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Test CheXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can load model from checkpoint\n",
    "#model = get_symbol(model_name='resnet50')\n",
    "# Load the saved paremeters into the instance\n",
    "#serializers.load_npz('my_mnist.model', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27 s, sys: 6.86 s, total: 33.9 s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_truth = test_dataset.labels\n",
    "y_guess = []\n",
    "test_iter.reset()\n",
    "with chainer.using_config('train', False), chainer.using_config('enable_backprop', False):\n",
    "    for test_batch in test_iter:\n",
    "        # Data\n",
    "        x_test, y_test = concat_examples(test_batch, device=DEVICES[0])\n",
    "        # Prediction (need to apply sigmoid to turn into probability)\n",
    "        pred = cuda.to_cpu(F.sigmoid(predictor(x_test)).data)\n",
    "        # Collect results\n",
    "        y_guess.append(pred)           \n",
    "# Concatenate\n",
    "y_guess = np.concatenate(y_guess, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full AUC [0.7898239277954502, 0.8376775446313578, 0.7568095698794695, 0.8524100729679737, 0.867159043399087, 0.9078994670721408, 0.7350985953693724, 0.8017389668333064, 0.583063570088059, 0.8163193902518544, 0.7154199872264138, 0.771522169766665, 0.6983676685804345, 0.8591604665524035]\n",
      "Test AUC: 0.7852\n"
     ]
    }
   ],
   "source": [
    "# Test AUC: 0.7993\n",
    "print(\"Test AUC: {0:.4f}\".format(compute_roc_auc(y_truth, y_guess)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
