{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# 0. Rewrite to use tf.estimator.Estimator()!\n",
    "\n",
    "\n",
    "# 1. Move func:get_imgloc_labels into common/utilities.py\n",
    "# 2. Move train/val/test split as function into common/utilities.py\n",
    "# 3. Move auc_roc function into common/utilities.py\n",
    "# 4. Change data-order from NHWC to NCHW to improve speed\n",
    "\n",
    "# 5. IMPORTANT: For inference the symbol probably requires a training:false flag\n",
    "# 6. IMPORTANT: See if tfrecords reduces IO latency\n",
    "# 7. IMPORTANT: Multi-gpu wrapper????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#wget -N http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n",
    "#tar -xvf resnet_v1_50_2016_08_28.tar.gz\n",
    "#rm resnet_v1_50_2016_08_28.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nets import densenet  # Download from https://github.com/pudae/tensorflow-densenet\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework.ops import convert_to_tensor\n",
    "from tensorflow.contrib.data import Iterator\n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import random\n",
    "from common.utils import *\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "print(\"CPUs: \", CPU_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "CLASSES = 14\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "LR = 0.0001  # Effective learning-rate will decrease as BATCHSIZE rises\n",
    "EPOCHS = 5\n",
    "BATCHSIZE = 64  # Chainer auto scales batch\n",
    "IMAGENET_RGB_MEAN = np.array([123.68, 116.78, 103.94], dtype=np.float32)\n",
    "IMAGENET_SCALE_FACTOR = 0.017\n",
    "TOT_PATIENT_NUMBER = 30805  # From data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CSV_DEST = \"chestxray\"\n",
    "IMAGE_FOLDER = os.path.join(CSV_DEST, \"images\")\n",
    "LABEL_FILE = os.path.join(CSV_DEST, \"Data_Entry_2017.csv\")\n",
    "print(IMAGE_FOLDER, LABEL_FILE)\n",
    "# Model checkpoint\n",
    "PRETRAINED_WEIGHTS = True\n",
    "CHKPOINT = 'tfdensenet/tf-densenet121.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Download data\n",
    "print(\"Please make sure to download\")\n",
    "print(\"https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\")\n",
    "download_data_chextxray(CSV_DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayData():\n",
    "    \n",
    "    def __init__(self, img_dir, lbl_file, patient_ids, mode='inference', \n",
    "                 width=WIDTH, height=HEIGHT, batch_size=BATCHSIZE, \n",
    "                 imagenet_mean=IMAGENET_RGB_MEAN, imagenet_scaling = IMAGENET_SCALE_FACTOR,\n",
    "                 shuffle=True):\n",
    "        # Get data\n",
    "        self.img_locs, self.labels = get_imgloc_labels(img_dir, lbl_file, patient_ids)\n",
    "        self.data_size = len(self.labels)\n",
    "        self.imagenet_mean = imagenet_mean\n",
    "        self.imagenet_scaling = imagenet_scaling\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        # Create dataset\n",
    "        # Performance: https://www.tensorflow.org/versions/master/performance/datasets_performance\n",
    "        # Following: https://stackoverflow.com/a/48096625/6772173\n",
    "        data = tf.data.Dataset.from_tensor_slices((self.img_locs, self.labels))\n",
    "        # Processing\n",
    "        if mode == 'training':\n",
    "            data = data.shuffle(self.data_size).map(self._parse_function_train,\n",
    "                            num_parallel_calls=CPU_COUNT).prefetch(10*batch_size).batch(batch_size)\n",
    "        else:\n",
    "            data = data.map(self._parse_function_inference,\n",
    "                            num_parallel_calls=CPU_COUNT).prefetch(10*batch_size).batch(batch_size)\n",
    "        \n",
    "        self.data = data        \n",
    "        print(\"Loaded {} labels and {} images\".format(len(self.labels), len(self.img_locs)))\n",
    "        \n",
    "        \n",
    "    def _parse_function_train(self, filename, label):\n",
    "        img_rgb, label = self._preprocess_image_labels(filename, label)\n",
    "        # Super high CPU usuage bottlenecking GPU\n",
    "        # Random crop\n",
    "        img_rgb = tf.image.resize_images(img_rgb, [self.height+40, self.width+40])\n",
    "        img_rgb = tf.random_crop(img_rgb, [self.height, self.width, 3])\n",
    "        # Random flip\n",
    "        img_rgb = tf.image.random_flip_left_right(img_rgb)\n",
    "        # Random rotation\n",
    "        rot_angle = np.random.randint(-10, 10)\n",
    "        img_rgb = tf.contrib.image.rotate(img_rgb, rot_angle)\n",
    "        return img_rgb, label\n",
    "        \n",
    "        \n",
    "    def _parse_function_inference(self, filename, label):\n",
    "        img_rgb, label = self._preprocess_image_labels(filename, label)\n",
    "        # Resize to final dimensions\n",
    "        img_rgb = tf.image.resize_images(img_rgb, [self.height, self.width])\n",
    "        return img_rgb, label \n",
    "       \n",
    "    \n",
    "    def _preprocess_image_labels(self, filename, label):\n",
    "        # load and preprocess the image\n",
    "        img_decoded = tf.to_float(tf.image.decode_png(tf.read_file(filename), channels=3))\n",
    "        img_centered = tf.subtract(img_decoded, self.imagenet_mean)\n",
    "        img_rgb = img_centered * self.imagenet_scaling\n",
    "        return img_rgb, label\n",
    "    \n",
    "    \n",
    "def get_imgloc_labels(img_dir, lbl_file, patient_ids):\n",
    "    \"\"\" Function to process data into a list of img_locs containing string paths\n",
    "    and labels, which are one-hot encoded.\n",
    "    \n",
    "    Move this to the common/utilities file\"\"\"\n",
    "    # Read labels-csv\n",
    "    df = pd.read_csv(lbl_file)\n",
    "    # Process\n",
    "    # Split labels on unfiltered data\n",
    "    df_label = df['Finding Labels'].str.split(\n",
    "        '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "    # Filter by patient-ids (both)\n",
    "    df_label['Patient ID'] = df['Patient ID']\n",
    "    df_label = df_label[df_label['Patient ID'].isin(patient_ids)]\n",
    "    df = df[df['Patient ID'].isin(patient_ids)]\n",
    "    # Remove unncessary columns\n",
    "    df_label.drop(['Patient ID','No Finding'], axis=1, inplace=True)  \n",
    "\n",
    "    # List of images (full-path)\n",
    "    img_locs =  df['Image Index'].map(lambda im: os.path.join(img_dir, im)).values\n",
    "    # One-hot encoded labels (float32 for BCE loss)\n",
    "    labels = df_label.values   \n",
    "    return img_locs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training / Valid / Test split (70% / 10% / 20%)\n",
    "train_set, other_set = train_test_split(\n",
    "    range(1,TOT_PATIENT_NUMBER+1), train_size=0.7, test_size=0.3, shuffle=False)\n",
    "valid_set, test_set = train_test_split(other_set, train_size=1/3, test_size=2/3, shuffle=False)\n",
    "print(\"train:{} valid:{} test:{}\".format(\n",
    "    len(train_set), len(valid_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    # Create dataset for iterator\n",
    "    train_dataset = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=train_set,  mode='training')\n",
    "    valid_dataset = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=valid_set, shuffle=False)\n",
    "    test_dataset  = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=test_set, shuffle=False)\n",
    "    \n",
    "    # Create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(train_dataset.data.output_types,\n",
    "                                       train_dataset.data.output_shapes)\n",
    "    next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol(model_name, in_tensor, \n",
    "               reuse=True, is_training=True, chkpoint=CHKPOINT, out_features=CLASSES):\n",
    "    \"\"\" Conver to dictionary lookup \"\"\"\n",
    "    if model_name == 'resnet50':\n",
    "        # Load variables into model (without this nothing is restored)\n",
    "        tf.train.get_or_create_global_step()\n",
    "        # Import symbol\n",
    "        with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "            base_model, _ = resnet_v1.resnet_v1_50(X, num_classes=None, \n",
    "                                                   is_training=is_training)\n",
    "        # Collect variables to restore from checkpoint\n",
    "        variables_to_restore = slim.get_variables_to_restore()\n",
    "        #print(variables_to_restore)\n",
    "        init_fn = slim.assign_from_checkpoint_fn(chkpoint, variables_to_restore)   \n",
    "        # Attach extra layers\n",
    "        fc = tf.layers.dense(base_model, out_features, name='output')\n",
    "        # Activation function will be included in loss\n",
    "        sym = tf.reshape(fc, shape=[-1, out_features])\n",
    "        \n",
    "    elif model_name == 'densenet121':\n",
    "        # Load variables into model (without this nothing is restored)\n",
    "        tf.train.get_or_create_global_step()\n",
    "        # Import symbol\n",
    "        dense_args = densenet.densenet_arg_scope()\n",
    "        #dense_args[data_format]='NCHW'\n",
    "        with slim.arg_scope(dense_args):\n",
    "            logits, _ = densenet.densenet121(X, num_classes=out_features, \n",
    "                                             is_training=is_training, reuse=reuse)\n",
    "        # Collect variables to restore from checkpoint\n",
    "        variables_to_restore = slim.get_variables_to_restore(\n",
    "            exclude=['densenet121/logits', 'predictions'])\n",
    "        #print(variables_to_restore)\n",
    "        init_fn = slim.assign_from_checkpoint_fn(chkpoint, variables_to_restore)  \n",
    "        # Reshape logits to (None, CLASSES) since my label is (None, CLASSES)\n",
    "        sym = tf.reshape(logits, shape=[-1, CLASSES])\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unknown model-name\")\n",
    "        \n",
    "    return sym, init_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_symbol(sym, out_tensor, lr=LR, multi_gpu=True):\n",
    "    loss_fn = tf.nn.sigmoid_cross_entropy_with_logits(logits=sym, labels=y)\n",
    "    loss = tf.reduce_mean(loss_fn)\n",
    "    optimizer = tf.train.AdamOptimizer(lr, beta1=0.9, beta2=0.999)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    return training_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars):\n",
    "        #print(\"Initialising: \", not_initialized_vars)\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_auc(data_gt, data_pd, full=True, classes=CLASSES):\n",
    "    # Push to util\n",
    "    roc_auc = []\n",
    "    for i in range(classes):\n",
    "        roc_auc.append(roc_auc_score(data_gt[:, i], data_pd[:, i]))\n",
    "    print(\"Full AUC\", roc_auc)\n",
    "    roc_auc = np.mean(roc_auc)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Place-holders\n",
    "sess = tf.Session()\n",
    "X = tf.placeholder(tf.float32, shape=[None, WIDTH, HEIGHT, CHANNELS])\n",
    "y = tf.placeholder(tf.float32, shape=[None, CLASSES])\n",
    "# Create symbol\n",
    "sym, init_fn = get_symbol(model_name='densenet121', in_tensor=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training operation\n",
    "model, loss = init_symbol(sym=sym, out_tensor=y)\n",
    "# Create iterator\n",
    "training_init_op = iterator.make_initializer(train_dataset.data)\n",
    "train_batches_per_epoch = int(np.floor(train_dataset.data_size/BATCHSIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Restoring parameters from tfdensenet/tf-densenet121.ckpt\n",
    "init_fn(sess)\n",
    "# Initialise uninitialised vars (FC layer & Adam)\n",
    "init_uninitialized(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Epoch number: 1\n",
    "Average loss: 0.1680990606546402\n",
    "Epoch time: 768 seconds\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Epoch number: 2\n",
    "Average loss: 0.1511792093515396\n",
    "Epoch time: 760 seconds\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(\"Epoch number: {}\".format(epoch+1))\n",
    "    # Logging\n",
    "    epoch_loss = []\n",
    "    stime = time.time()\n",
    "    # Initialize iterator with the training dataset\n",
    "    sess.run(training_init_op)\n",
    "    \n",
    "    for step in range(train_batches_per_epoch):\n",
    "        \n",
    "        # get next batch of data\n",
    "        img_batch, label_batch = sess.run(next_batch)\n",
    "        # And run the training op\n",
    "        _, loss_tr = sess.run([model, loss], feed_dict={X: img_batch, y: label_batch})\n",
    "        epoch_loss.append(loss_tr)\n",
    "        \n",
    "    etime = time.time()\n",
    "    print(\"Average loss: {}\".format(np.mean(epoch_loss)))\n",
    "    # 7min20s for chainer\n",
    "    print(\"Epoch time: {0:.0f} seconds\".format(etime-stime))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph for testing\n",
    "sym_test, _ = get_symbol(model_name='densenet121', in_tensor=X, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test\n",
    "testing_init_op = iterator.make_initializer(test_dataset.data)\n",
    "sess.run(testing_init_op)\n",
    "\n",
    "test_batches_per_epoch = int(np.floor(test_dataset.data_size/BATCHSIZE))\n",
    "pred = tf.sigmoid(sym_test)\n",
    "y_guess = []\n",
    "\n",
    "for step in range(test_batches_per_epoch):\n",
    "    # get next batch of data\n",
    "    img_batch, _ = sess.run(next_batch)\n",
    "    output = sess.run(pred, feed_dict={X: img_batch})\n",
    "    y_guess.append(output)\n",
    "        \n",
    "# Concatenate\n",
    "y_guess = np.concatenate(y_guess, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_truth = test_dataset.labels\n",
    "y_truth = y_truth[:len(y_guess)]  # Iterator only returns complete batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test AUC: {0:.4f}\".format(compute_roc_auc(y_truth, y_guess)))\n",
    "# 0.7755 if training:False\n",
    "# x if training-flag omitted -> no effect\n",
    "# Test AUC: 0.6500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
