{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#wget -N http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz\n",
    "#tar -xvf resnet_v1_50_2016_08_28.tar.gz\n",
    "#rm resnet_v1_50_2016_08_28.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from nets import densenet  # Download from https://github.com/pudae/tensorflow-densenet\n",
    "from tensorflow.contrib.slim.nets import resnet_v1\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "from tensorflow.contrib.data import Dataset\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework.ops import convert_to_tensor\n",
    "from tensorflow.contrib.data import Iterator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import random\n",
    "#import cv2\n",
    "from common.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "CLASSES = 14\n",
    "WIDTH = 224\n",
    "HEIGHT = 224\n",
    "CHANNELS = 3\n",
    "LR = 0.0001  # Effective learning-rate will decrease as BATCHSIZE rises\n",
    "EPOCHS = 5\n",
    "#BATCHSIZE = 64*NUM_GPUS\n",
    "BATCHSIZE = 64  # Chainer auto scales batch\n",
    "IMAGENET_RGB_MEAN =  np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMAGENET_RGB_SD =  np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "TOT_PATIENT_NUMBER = 30805  # From data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestxray/images chestxray/Data_Entry_2017.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "CSV_DEST = \"chestxray\"\n",
    "IMAGE_FOLDER = os.path.join(CSV_DEST, \"images\")\n",
    "LABEL_FILE = os.path.join(CSV_DEST, \"Data_Entry_2017.csv\")\n",
    "print(IMAGE_FOLDER, LABEL_FILE)\n",
    "# Model checkpoint\n",
    "PRETRAINED_WEIGHTS = True\n",
    "#CHKPOINT = 'resnet_v1_50.ckpt' \n",
    "CHKPOINT = 'tfdensenet/tf-densenet121.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please make sure to download\n",
      "https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\n",
      "Data already exists\n",
      "CPU times: user 813 ms, sys: 332 ms, total: 1.15 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Download data\n",
    "print(\"Please make sure to download\")\n",
    "print(\"https://docs.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-linux#download-and-install-azcopy\")\n",
    "download_data_chextxray(CSV_DEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayData():\n",
    "    \n",
    "    def __init__(self, img_dir, lbl_file, patient_ids, mode='inference', \n",
    "                 width=WIDTH, height=HEIGHT, batch_size=BATCHSIZE,\n",
    "                 shuffle=True, buffer_size=20):\n",
    "        \n",
    "        # Following: https://github.com/kratzert/finetune_alexnet_with_tensorflow/blob/master/datagenerator.py\n",
    "        \n",
    "        # Read labels-csv\n",
    "        df = pd.read_csv(lbl_file)\n",
    "        \n",
    "        # Process\n",
    "        # Split labels on unfiltered data\n",
    "        df_label = df['Finding Labels'].str.split(\n",
    "            '|', expand=False).str.join(sep='*').str.get_dummies(sep='*')\n",
    "        # Filter by patient-ids (both)\n",
    "        df_label['Patient ID'] = df['Patient ID']\n",
    "        df_label = df_label[df_label['Patient ID'].isin(patient_ids)]\n",
    "        df = df[df['Patient ID'].isin(patient_ids)]\n",
    "        # Remove unncessary columns\n",
    "        df_label.drop(['Patient ID','No Finding'], axis=1, inplace=True)  \n",
    "        \n",
    "        # List of images (full-path)\n",
    "        self.img_locs =  df['Image Index'].map(lambda im: os.path.join(img_dir, im)).values\n",
    "        # One-hot encoded labels (float32 for BCE loss)\n",
    "        self.labels = df_label.values      \n",
    "        \n",
    "        # Number of samples in datatset\n",
    "        self.data_size = len(self.labels)\n",
    "        \n",
    "        # Create dataset\n",
    "        data = Dataset.from_tensor_slices((self.img_locs, self.labels))\n",
    "        \n",
    "        # distinguish between train/infer. when calling the parsing functions\n",
    "        if mode == 'training':\n",
    "            data = data.map(self._parse_function_train, num_threads=8,\n",
    "                      output_buffer_size=20*batch_size)\n",
    "        elif mode == 'inference':\n",
    "            data = data.map(self._parse_function_train, num_threads=8,\n",
    "                      output_buffer_size=20*batch_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode '%s'.\" % (mode))\n",
    "\n",
    "        # shuffle the first `buffer_size` elements of the dataset\n",
    "        if shuffle:\n",
    "            data = data.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "        # create a new dataset with batches of images\n",
    "        data = data.batch(batch_size)\n",
    "\n",
    "        self.data = data        \n",
    "        print(\"Loaded {} labels and {} images\".format(len(self.labels), len(self.img_locs)))\n",
    "\n",
    "     \n",
    "    def _parse_function_train(self, filename, label):\n",
    "\n",
    "        # load and preprocess the image\n",
    "        img_string = tf.read_file(filename)\n",
    "        img_decoded = tf.image.decode_png(img_string, channels=3)\n",
    "        img_resized = tf.image.resize_images(img_decoded, [224, 224])\n",
    "        img_centered = tf.subtract(img_resized, IMAGENET_RGB_MEAN)\n",
    "\n",
    "        # RGB -> BGR\n",
    "        img_bgr = img_centered[:, :, ::-1]\n",
    "\n",
    "        return img_bgr, label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:21563 valid:3080 test:6162\n"
     ]
    }
   ],
   "source": [
    "# Training / Valid / Test split (70% / 10% / 20%)\n",
    "train_set, other_set = train_test_split(\n",
    "    range(1,TOT_PATIENT_NUMBER+1), train_size=0.7, test_size=0.3, shuffle=False)\n",
    "valid_set, test_set = train_test_split(other_set, train_size=1/3, test_size=2/3, shuffle=False)\n",
    "print(\"train:{} valid:{} test:{}\".format(\n",
    "    len(train_set), len(valid_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-e719ead1da2c>:32: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From <ipython-input-7-e719ead1da2c>:37: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with num_threads is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "WARNING:tensorflow:From <ipython-input-7-e719ead1da2c>:37: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "Loaded 87306 labels and 87306 images\n",
      "Loaded 7616 labels and 7616 images\n",
      "Loaded 17198 labels and 17198 images\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    train_dataset = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=train_set,  mode='training')\n",
    "    valid_dataset = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=valid_set, shuffle=False)\n",
    "    test_dataset  = XrayData(img_dir=IMAGE_FOLDER, lbl_file=LABEL_FILE, patient_ids=test_set, shuffle=False)\n",
    "    \n",
    "    # create an reinitializable iterator given the dataset structure\n",
    "    iterator = Iterator.from_structure(train_dataset.data.output_types,\n",
    "                                       train_dataset.data.output_shapes)\n",
    "    next_batch = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbol(model_name, in_tensor, chkpoint=CHKPOINT, out_features=CLASSES):\n",
    "    if model_name == 'resnet50':\n",
    "        # Load variables into model (without this nothing is restored)\n",
    "        tf.train.get_or_create_global_step()\n",
    "        # Import symbol\n",
    "        with slim.arg_scope(resnet_v1.resnet_arg_scope()):\n",
    "            base_model, _ = resnet_v1.resnet_v1_50(X, None, is_training=True)\n",
    "        # Collect variables to restore from checkpoint\n",
    "        variables_to_restore = slim.get_variables_to_restore()\n",
    "        #print(variables_to_restore)\n",
    "        init_fn = slim.assign_from_checkpoint_fn(chkpoint, variables_to_restore)   \n",
    "        # Attach extra layers\n",
    "        fc = tf.layers.dense(base_model, out_features, name='output')\n",
    "        # Activation function will be included in loss\n",
    "        sym = tf.reshape(fc, shape=[-1, out_features])\n",
    "        \n",
    "    elif model_name == 'densenet121':\n",
    "        # Load variables into model (without this nothing is restored)\n",
    "        tf.train.get_or_create_global_step()\n",
    "        # Import symbol\n",
    "        dense_args = densenet.densenet_arg_scope()\n",
    "        #print(dense_args)  # Add NCHW later\n",
    "        with slim.arg_scope(dense_args):\n",
    "            logits, _ = densenet.densenet121(X, num_classes=out_features, is_training=True, reuse=None)\n",
    "        # Collect variables to restore from checkpoint\n",
    "        variables_to_restore = slim.get_variables_to_restore(\n",
    "            exclude=['densenet121/logits', 'predictions'])\n",
    "        #print(variables_to_restore)\n",
    "        init_fn = slim.assign_from_checkpoint_fn(chkpoint, variables_to_restore)  \n",
    "        # Reshape logits to (None, CLASSES) since my label is (None, CLASSES)\n",
    "        sym = tf.reshape(logits, shape=[-1, CLASSES])\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unknown model-name\")\n",
    "        \n",
    "    return sym, init_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_symbol(sym, out_tensor, lr=LR):\n",
    "    loss_fn = tf.nn.sigmoid_cross_entropy_with_logits(logits=sym, labels=y)\n",
    "    loss = tf.reduce_mean(loss_fn)\n",
    "    optimizer = tf.train.AdamOptimizer(lr, beta1=0.9, beta2=0.999)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    return training_op, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v, f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars):\n",
    "        #print(\"Initialising: \", not_initialized_vars)\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 132 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Place-holders\n",
    "X = tf.placeholder(tf.float32, shape=[None, WIDTH, HEIGHT, CHANNELS])\n",
    "y = tf.placeholder(tf.float32, shape=[None, CLASSES])\n",
    "\n",
    "# Create symbol\n",
    "sym, init_fn = get_symbol(model_name='densenet121', in_tensor=X)\n",
    "\n",
    "# Create training operation\n",
    "model, loss = init_symbol(sym=sym, out_tensor=y)\n",
    "training_init_op = iterator.make_initializer(train_dataset.data)\n",
    "train_batches_per_epoch = int(np.floor(train_dataset.data_size/BATCHSIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained weights\n",
      "INFO:tensorflow:Restoring parameters from tfdensenet/tf-densenet121.ckpt\n",
      "CPU times: user 5.21 s, sys: 1.47 s, total: 6.67 s\n",
      "Wall time: 6.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Launch session and load model from checkpoint\n",
    "sess = tf.Session()\n",
    "\n",
    "# Temp\n",
    "if PRETRAINED_WEIGHTS:\n",
    "    print(\"Loading pre-trained weights\")\n",
    "    init_fn(sess)  # Load from checkpoint\n",
    "\n",
    "# Initialise uninitialised vars (FC layer & Adam)\n",
    "init_uninitialized(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epoch number: 1\n",
    "#Average loss: 0.1651265025138855\n",
    "#Epoch time: 510 seconds\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#Epoch number: 2\n",
    "#Average loss: 0.14739550650119781\n",
    "#Epoch time: 511 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1\n",
      "Average loss: 0.17311111092567444\n",
      "Epoch time: 702 seconds\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Epoch number: 2\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(\"Epoch number: {}\".format(epoch+1))\n",
    "    # Logging\n",
    "    epoch_loss = []\n",
    "    stime = time.time()\n",
    "    # Initialize iterator with the training dataset\n",
    "    sess.run(training_init_op)\n",
    "    \n",
    "    for step in range(train_batches_per_epoch):\n",
    "        \n",
    "        # get next batch of data\n",
    "        img_batch, label_batch = sess.run(next_batch)\n",
    "        # And run the training op\n",
    "        _, loss_val = sess.run([model, loss], feed_dict={X: img_batch, y: label_batch})\n",
    "        epoch_loss.append(loss_val)\n",
    "        \n",
    "    etime = time.time()\n",
    "    print(\"Average loss: {}\".format(np.mean(epoch_loss)))\n",
    "    # 7min20s for chainer\n",
    "    print(\"Epoch time: {0:.0f} seconds\".format(etime-stime))\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
